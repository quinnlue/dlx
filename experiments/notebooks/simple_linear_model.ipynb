{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948e45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../../../\")\n",
    "\n",
    "from src.core.module import Module, Linear, LayerNorm\n",
    "from src.core.losses import CrossEntropyWithLogits, BinaryCrossEntropyWithLogits\n",
    "from src.core.optim import Standard, AdamW\n",
    "from src.core.tensor import Tensor\n",
    "from src.utils.lr_scheduler import LRScheduler\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List\n",
    "# from src.tokenizer.tokenizer import Tokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11bbef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.7861328125\n",
      "Epoch 19, Loss: 0.775390625\n",
      "Epoch 29, Loss: 0.7626953125\n",
      "Epoch 39, Loss: 0.7509765625\n",
      "Epoch 49, Loss: 0.73779296875\n",
      "Epoch 59, Loss: 0.72509765625\n",
      "Epoch 69, Loss: 0.71240234375\n",
      "Epoch 79, Loss: 0.69921875\n",
      "Epoch 89, Loss: 0.6875\n",
      "Epoch 99, Loss: 0.6748046875\n",
      "Epoch 109, Loss: 0.6630859375\n",
      "Epoch 119, Loss: 0.650390625\n",
      "Epoch 129, Loss: 0.6396484375\n",
      "Epoch 139, Loss: 0.6279296875\n",
      "Epoch 149, Loss: 0.6162109375\n",
      "Epoch 159, Loss: 0.60595703125\n",
      "Epoch 169, Loss: 0.59521484375\n",
      "Epoch 179, Loss: 0.5849609375\n",
      "Epoch 189, Loss: 0.57470703125\n",
      "Epoch 199, Loss: 0.56494140625\n",
      "Epoch 209, Loss: 0.55517578125\n",
      "Epoch 219, Loss: 0.5458984375\n",
      "Epoch 229, Loss: 0.537109375\n",
      "Epoch 239, Loss: 0.5283203125\n",
      "Epoch 249, Loss: 0.51953125\n",
      "Epoch 259, Loss: 0.51123046875\n",
      "Epoch 269, Loss: 0.50341796875\n",
      "Epoch 279, Loss: 0.495361328125\n",
      "Epoch 289, Loss: 0.48779296875\n",
      "Epoch 299, Loss: 0.47998046875\n",
      "Epoch 309, Loss: 0.47314453125\n",
      "Epoch 319, Loss: 0.466064453125\n",
      "Epoch 329, Loss: 0.459228515625\n",
      "Epoch 339, Loss: 0.45263671875\n",
      "Epoch 349, Loss: 0.4462890625\n",
      "Epoch 359, Loss: 0.43994140625\n",
      "Epoch 369, Loss: 0.43408203125\n",
      "Epoch 379, Loss: 0.427734375\n",
      "Epoch 389, Loss: 0.42236328125\n",
      "Epoch 399, Loss: 0.41650390625\n",
      "Epoch 409, Loss: 0.411376953125\n",
      "Epoch 419, Loss: 0.40576171875\n",
      "Epoch 429, Loss: 0.400634765625\n",
      "Epoch 439, Loss: 0.3955078125\n",
      "Epoch 449, Loss: 0.390869140625\n",
      "Epoch 459, Loss: 0.385986328125\n",
      "Epoch 469, Loss: 0.38134765625\n",
      "Epoch 479, Loss: 0.377197265625\n",
      "Epoch 489, Loss: 0.37255859375\n",
      "Epoch 499, Loss: 0.368408203125\n",
      "Epoch 509, Loss: 0.364013671875\n",
      "Epoch 519, Loss: 0.360107421875\n",
      "Epoch 529, Loss: 0.356201171875\n",
      "Epoch 539, Loss: 0.3525390625\n",
      "Epoch 549, Loss: 0.348876953125\n",
      "Epoch 559, Loss: 0.3447265625\n",
      "Epoch 569, Loss: 0.341552734375\n",
      "Epoch 579, Loss: 0.338134765625\n",
      "Epoch 589, Loss: 0.334716796875\n",
      "Epoch 599, Loss: 0.3310546875\n",
      "Epoch 609, Loss: 0.328125\n",
      "Epoch 619, Loss: 0.324951171875\n",
      "Epoch 629, Loss: 0.322021484375\n",
      "Epoch 639, Loss: 0.31884765625\n",
      "Epoch 649, Loss: 0.316162109375\n",
      "Epoch 659, Loss: 0.31298828125\n",
      "Epoch 669, Loss: 0.310302734375\n",
      "Epoch 679, Loss: 0.3076171875\n",
      "Epoch 689, Loss: 0.304931640625\n",
      "Epoch 699, Loss: 0.302490234375\n",
      "Epoch 709, Loss: 0.2998046875\n",
      "Epoch 719, Loss: 0.29736328125\n",
      "Epoch 729, Loss: 0.294921875\n",
      "Epoch 739, Loss: 0.29248046875\n",
      "Epoch 749, Loss: 0.290283203125\n",
      "Epoch 759, Loss: 0.2880859375\n",
      "Epoch 769, Loss: 0.28564453125\n",
      "Epoch 779, Loss: 0.283447265625\n",
      "Epoch 789, Loss: 0.28125\n",
      "Epoch 799, Loss: 0.279296875\n",
      "Epoch 809, Loss: 0.27734375\n",
      "Epoch 819, Loss: 0.275146484375\n",
      "Epoch 829, Loss: 0.273193359375\n",
      "Epoch 839, Loss: 0.271240234375\n",
      "Epoch 849, Loss: 0.26904296875\n",
      "Epoch 859, Loss: 0.267578125\n",
      "Epoch 869, Loss: 0.265625\n",
      "Epoch 879, Loss: 0.263916015625\n",
      "Epoch 889, Loss: 0.26220703125\n",
      "Epoch 899, Loss: 0.260498046875\n",
      "Epoch 909, Loss: 0.2587890625\n",
      "Epoch 919, Loss: 0.2568359375\n",
      "Epoch 929, Loss: 0.255615234375\n",
      "Epoch 939, Loss: 0.25390625\n",
      "Epoch 949, Loss: 0.251953125\n",
      "Epoch 959, Loss: 0.250732421875\n",
      "Epoch 969, Loss: 0.2491455078125\n",
      "Epoch 979, Loss: 0.2476806640625\n",
      "Epoch 989, Loss: 0.24609375\n",
      "Epoch 999, Loss: 0.2447509765625\n",
      "Epoch 1009, Loss: 0.2431640625\n",
      "Epoch 1019, Loss: 0.2418212890625\n",
      "Epoch 1029, Loss: 0.240478515625\n",
      "Epoch 1039, Loss: 0.2391357421875\n",
      "Epoch 1049, Loss: 0.2379150390625\n",
      "Epoch 1059, Loss: 0.236572265625\n",
      "Epoch 1069, Loss: 0.235107421875\n",
      "Epoch 1079, Loss: 0.23388671875\n",
      "Epoch 1089, Loss: 0.2325439453125\n",
      "Epoch 1099, Loss: 0.2313232421875\n",
      "Epoch 1109, Loss: 0.2301025390625\n",
      "Epoch 1119, Loss: 0.22900390625\n",
      "Epoch 1129, Loss: 0.2276611328125\n",
      "Epoch 1139, Loss: 0.2265625\n",
      "Epoch 1149, Loss: 0.2255859375\n",
      "Epoch 1159, Loss: 0.224365234375\n",
      "Epoch 1169, Loss: 0.22314453125\n",
      "Epoch 1179, Loss: 0.22216796875\n",
      "Epoch 1189, Loss: 0.220947265625\n",
      "Epoch 1199, Loss: 0.2197265625\n",
      "Epoch 1209, Loss: 0.21875\n",
      "Epoch 1219, Loss: 0.2177734375\n",
      "Epoch 1229, Loss: 0.2166748046875\n",
      "Epoch 1239, Loss: 0.215576171875\n",
      "Epoch 1249, Loss: 0.2147216796875\n",
      "Epoch 1259, Loss: 0.213623046875\n",
      "Epoch 1269, Loss: 0.2125244140625\n",
      "Epoch 1279, Loss: 0.211669921875\n",
      "Epoch 1289, Loss: 0.2105712890625\n",
      "Epoch 1299, Loss: 0.209716796875\n",
      "Epoch 1309, Loss: 0.208740234375\n",
      "Epoch 1319, Loss: 0.2076416015625\n",
      "Epoch 1329, Loss: 0.2069091796875\n",
      "Epoch 1339, Loss: 0.205810546875\n",
      "Epoch 1349, Loss: 0.205078125\n",
      "Epoch 1359, Loss: 0.2041015625\n",
      "Epoch 1369, Loss: 0.2032470703125\n",
      "Epoch 1379, Loss: 0.2021484375\n",
      "Epoch 1389, Loss: 0.201416015625\n",
      "Epoch 1399, Loss: 0.2003173828125\n",
      "Epoch 1409, Loss: 0.1995849609375\n",
      "Epoch 1419, Loss: 0.1988525390625\n",
      "Epoch 1429, Loss: 0.1978759765625\n",
      "Epoch 1439, Loss: 0.197021484375\n",
      "Epoch 1449, Loss: 0.1962890625\n",
      "Epoch 1459, Loss: 0.1953125\n",
      "Epoch 1469, Loss: 0.194580078125\n",
      "Epoch 1479, Loss: 0.19384765625\n",
      "Epoch 1489, Loss: 0.19287109375\n",
      "Epoch 1499, Loss: 0.1922607421875\n",
      "Epoch 1509, Loss: 0.19140625\n",
      "Epoch 1519, Loss: 0.1904296875\n",
      "Epoch 1529, Loss: 0.1898193359375\n",
      "Epoch 1539, Loss: 0.1890869140625\n",
      "Epoch 1549, Loss: 0.1881103515625\n",
      "Epoch 1559, Loss: 0.1873779296875\n",
      "Epoch 1569, Loss: 0.186767578125\n",
      "Epoch 1579, Loss: 0.18603515625\n",
      "Epoch 1589, Loss: 0.185302734375\n",
      "Epoch 1599, Loss: 0.1844482421875\n",
      "Epoch 1609, Loss: 0.1837158203125\n",
      "Epoch 1619, Loss: 0.1829833984375\n",
      "Epoch 1629, Loss: 0.182373046875\n",
      "Epoch 1639, Loss: 0.181640625\n",
      "Epoch 1649, Loss: 0.1807861328125\n",
      "Epoch 1659, Loss: 0.18017578125\n",
      "Epoch 1669, Loss: 0.179443359375\n",
      "Epoch 1679, Loss: 0.1787109375\n",
      "Epoch 1689, Loss: 0.1781005859375\n",
      "Epoch 1699, Loss: 0.17724609375\n",
      "Epoch 1709, Loss: 0.176513671875\n",
      "Epoch 1719, Loss: 0.1759033203125\n",
      "Epoch 1729, Loss: 0.17529296875\n",
      "Epoch 1739, Loss: 0.1746826171875\n",
      "Epoch 1749, Loss: 0.174072265625\n",
      "Epoch 1759, Loss: 0.17333984375\n",
      "Epoch 1769, Loss: 0.172607421875\n",
      "Epoch 1779, Loss: 0.172119140625\n",
      "Epoch 1789, Loss: 0.17138671875\n",
      "Epoch 1799, Loss: 0.170654296875\n",
      "Epoch 1809, Loss: 0.169921875\n",
      "Epoch 1819, Loss: 0.16943359375\n",
      "Epoch 1829, Loss: 0.1685791015625\n",
      "Epoch 1839, Loss: 0.16796875\n",
      "Epoch 1849, Loss: 0.1673583984375\n",
      "Epoch 1859, Loss: 0.166748046875\n",
      "Epoch 1869, Loss: 0.1661376953125\n",
      "Epoch 1879, Loss: 0.16552734375\n",
      "Epoch 1889, Loss: 0.1649169921875\n",
      "Epoch 1899, Loss: 0.164306640625\n",
      "Epoch 1909, Loss: 0.1636962890625\n",
      "Epoch 1919, Loss: 0.1630859375\n",
      "Epoch 1929, Loss: 0.16259765625\n",
      "Epoch 1939, Loss: 0.161865234375\n",
      "Epoch 1949, Loss: 0.1612548828125\n",
      "Epoch 1959, Loss: 0.160888671875\n",
      "Epoch 1969, Loss: 0.16015625\n",
      "Epoch 1979, Loss: 0.1597900390625\n",
      "Epoch 1989, Loss: 0.1591796875\n",
      "Epoch 1999, Loss: 0.1585693359375\n",
      "Epoch 2009, Loss: 0.1580810546875\n",
      "Epoch 2019, Loss: 0.157470703125\n",
      "Epoch 2029, Loss: 0.1568603515625\n",
      "Epoch 2039, Loss: 0.1563720703125\n",
      "Epoch 2049, Loss: 0.15576171875\n",
      "Epoch 2059, Loss: 0.1552734375\n",
      "Epoch 2069, Loss: 0.1546630859375\n",
      "Epoch 2079, Loss: 0.154052734375\n",
      "Epoch 2089, Loss: 0.153564453125\n",
      "Epoch 2099, Loss: 0.153076171875\n",
      "Epoch 2109, Loss: 0.152587890625\n",
      "Epoch 2119, Loss: 0.152099609375\n",
      "Epoch 2129, Loss: 0.1514892578125\n",
      "Epoch 2139, Loss: 0.1510009765625\n",
      "Epoch 2149, Loss: 0.150390625\n",
      "Epoch 2159, Loss: 0.1500244140625\n",
      "Epoch 2169, Loss: 0.1494140625\n",
      "Epoch 2179, Loss: 0.14892578125\n",
      "Epoch 2189, Loss: 0.1484375\n",
      "Epoch 2199, Loss: 0.14794921875\n",
      "Epoch 2209, Loss: 0.1474609375\n",
      "Epoch 2219, Loss: 0.14697265625\n",
      "Epoch 2229, Loss: 0.1463623046875\n",
      "Epoch 2239, Loss: 0.14599609375\n",
      "Epoch 2249, Loss: 0.1455078125\n",
      "Epoch 2259, Loss: 0.14501953125\n",
      "Epoch 2269, Loss: 0.14453125\n",
      "Epoch 2279, Loss: 0.14404296875\n",
      "Epoch 2289, Loss: 0.1435546875\n",
      "Epoch 2299, Loss: 0.14306640625\n",
      "Epoch 2309, Loss: 0.142578125\n",
      "Epoch 2319, Loss: 0.1422119140625\n",
      "Epoch 2329, Loss: 0.1417236328125\n",
      "Epoch 2339, Loss: 0.1412353515625\n",
      "Epoch 2349, Loss: 0.140869140625\n",
      "Epoch 2359, Loss: 0.1402587890625\n",
      "Epoch 2369, Loss: 0.139892578125\n",
      "Epoch 2379, Loss: 0.139404296875\n",
      "Epoch 2389, Loss: 0.1390380859375\n",
      "Epoch 2399, Loss: 0.1385498046875\n",
      "Epoch 2409, Loss: 0.13818359375\n",
      "Epoch 2419, Loss: 0.1376953125\n",
      "Epoch 2429, Loss: 0.1373291015625\n",
      "Epoch 2439, Loss: 0.136962890625\n",
      "Epoch 2449, Loss: 0.1363525390625\n",
      "Epoch 2459, Loss: 0.135986328125\n",
      "Epoch 2469, Loss: 0.1356201171875\n",
      "Epoch 2479, Loss: 0.1351318359375\n",
      "Epoch 2489, Loss: 0.1348876953125\n",
      "Epoch 2499, Loss: 0.13427734375\n"
     ]
    }
   ],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = self.linear(7, 28, name=\"fc1\")\n",
    "        self.fc2 = self.linear(28, 2, name=\"fc2\")\n",
    "        self.ln = self.layer_norm(axis=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self, x: Tensor, y: Tensor, optimizer, num_epochs=100):\n",
    "        for epoch in range(num_epochs):\n",
    "            y_hat = self.forward(x)\n",
    "            \n",
    "            loss = CrossEntropyWithLogits(y_hat, y) * 0.1\n",
    "            \n",
    "            loss.backward()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.data * 10}\")\n",
    "                optimizer.step()\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../../src/experiments/data.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df['Quality'] = df['Quality'].apply(lambda x: 1 if x == \"Good\" else 0)\n",
    "X = Tensor(np.array(df.drop('Quality', axis=1).values))[:128]\n",
    "y = Tensor(np.array(df['Quality'].values))[:128] \n",
    "\n",
    "X_test = Tensor(np.array(df.drop('Quality', axis=1).values))[128:]\n",
    "y_test = Tensor(np.array(df['Quality'].values)).reshape((-1, 1))[128:]\n",
    "net = Net()\n",
    "\n",
    "net._build(X.shape)\n",
    "optimizer = AdamW(net.parameters(), lr=0.001, clip_norm=100.0)\n",
    "\n",
    "net.train(X, y, optimizer, num_epochs=2500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch 9, Loss: 1.07421875\n",
    "# Epoch 19, Loss: 1.060546875\n",
    "# Epoch 29, Loss: 1.0439453125\n",
    "# Epoch 39, Loss: 1.0283203125\n",
    "# Epoch 49, Loss: 1.01171875\n",
    "# Epoch 59, Loss: 0.99462890625\n",
    "# Epoch 69, Loss: 0.9775390625\n",
    "# Epoch 79, Loss: 0.9609375\n",
    "# Epoch 89, Loss: 0.9443359375\n",
    "# Epoch 99, Loss: 0.92822265625\n",
    "# Epoch 109, Loss: 0.91162109375\n",
    "# Epoch 119, Loss: 0.89599609375\n",
    "# Epoch 129, Loss: 0.8798828125\n",
    "# Epoch 139, Loss: 0.8642578125\n",
    "# Epoch 149, Loss: 0.84912109375\n",
    "# Epoch 159, Loss: 0.833984375\n",
    "# Epoch 169, Loss: 0.81884765625\n",
    "# Epoch 179, Loss: 0.8046875\n",
    "# Epoch 189, Loss: 0.7900390625\n",
    "# Epoch 199, Loss: 0.77587890625\n",
    "# Epoch 209, Loss: 0.76171875\n",
    "# Epoch 219, Loss: 0.74853515625\n",
    "# Epoch 229, Loss: 0.73486328125\n",
    "# Epoch 239, Loss: 0.7216796875\n",
    "# Epoch 249, Loss: 0.708984375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "62cf290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3029\n",
      "Epoch 10, Loss: 1.1438\n",
      "Epoch 20, Loss: 1.0108\n",
      "Epoch 30, Loss: 0.8985\n",
      "Epoch 40, Loss: 0.8017\n",
      "Epoch 50, Loss: 0.7168\n",
      "Epoch 60, Loss: 0.6414\n",
      "Epoch 70, Loss: 0.5736\n",
      "Epoch 80, Loss: 0.5124\n",
      "Epoch 90, Loss: 0.4572\n",
      "Epoch 100, Loss: 0.4080\n",
      "Epoch 110, Loss: 0.3646\n",
      "Epoch 120, Loss: 0.3270\n",
      "Epoch 130, Loss: 0.2947\n",
      "Epoch 140, Loss: 0.2674\n",
      "Epoch 150, Loss: 0.2442\n",
      "Epoch 160, Loss: 0.2246\n",
      "Epoch 170, Loss: 0.2080\n",
      "Epoch 180, Loss: 0.1938\n",
      "Epoch 190, Loss: 0.1817\n",
      "Epoch 200, Loss: 0.1711\n",
      "Epoch 210, Loss: 0.1617\n",
      "Epoch 220, Loss: 0.1534\n",
      "Epoch 230, Loss: 0.1459\n",
      "Epoch 240, Loss: 0.1392\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# Define the model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 28)\n",
    "        self.fc4 = nn.Linear(28, 3)  # 3 classes\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x  # raw logits, no sigmoid\n",
    "\n",
    "# Load + prep data\n",
    "df = pd.read_csv(\"../../../src/experiments/data.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Map classes properly starting at 0\n",
    "df['Quality'] = df['Quality'].apply(lambda x: 0 if x == \"Good\" else (1 if x == \"Bad\" else 2))  # Adjust if you have 3 classes\n",
    "\n",
    "X = torch.tensor(df.drop('Quality', axis=1).values, dtype=torch.float32)\n",
    "y = torch.tensor(df['Quality'].values, dtype=torch.long)  # LONG for CE loss!\n",
    "\n",
    "# Train/test split\n",
    "X_train, y_train = X[:128], y[:128]\n",
    "X_test, y_test = X[128:], y[128:]\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(250):\n",
    "    model.train()\n",
    "\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427cf5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
